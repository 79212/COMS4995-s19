<!DOCTYPE html>
<html>
  <head>
    <title>Recap & Summary</title>
    <meta charset="utf-8">
        <style>
       @import url(https://fonts.googleapis.com/css?family=Garamond);
       @import url(https://fonts.googleapis.com/css?family=Muli:400,700,400italic);
       @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
     </style>
     <link rel="stylesheet" href="../style.css">
  </head>
  <body>
    <textarea id="source">

class: center, middle

### W4995 Applied Machine Learning

# Recap & Summary

04/20/19

Andreas C. Müller

???
Only gonna talk about stuff for final, so not including first 11 lectures.
Also: this lecture will not be comprehensive, there will be things in the exam not in this lecture.
---
# Model inspection
???
# FIXME post-hoc vs interpretable model
---
# Types of explanations

## Explain "kind of model"
- How would the model change if this feature was dropped?
- Doesn't explain this particular fitted model

##Explain model globally
- How does the output depend on the input?
- Often: some form of marginals


## Explain model locally
- Why did it classify this point this way?
- Explanation could look like a "global" one but be different for each point.
- "What is the minimum change to classify it differently?"

???
Marginals: how does the prediction change as function of a particular Features?
---
# Permutation importance
Idea: measure marginal influence of one feature

```python
def permutation_importance(est, X, y, n_bootstrap=100):
  baseline_score = estimator.score(X, y)
  for f_idx in range(X.shape[1]):
      for b_idx in range(n_bootstrap):
          X_new = X.copy()
          X_new[:, f_idx] = np.random.shuffle(X[:, f_idx])
          feature_score = estimator.score(X_new, y)
          scores[f_idx, b_idx] = baseline_score - feature_score
```

--

- Applied on validation set given trained estimator.
- Also kinda slow.
- Implementation: ELI5,<br> https://github.com/scikit-learn/scikit-learn/pull/13146
- For RF: https://github.com/parrt/random-forest-importances (uses oob)
---
---

# Partial Dependence Plots
- Marginal dependence of prediction on one or two features

.smaller[
```python
from sklearn.ensemble.partial_dependence import plot_partial_dependence
boston = load_boston()
X_train, X_test, y_train, y_test = train_test_split(
    boston.data, boston.target,random_state=0)

gbrt = GradientBoostingRegressor().fit(X_train, y_train)

fig, axs = plot_partial_dependence(
    gbrt, X_train, np.argsort(gbrt.feature_importances_)[-6:],
    feature_names=boston.feature_names, n_jobs=3,
    grid_resolution=50)
```
]

???
But there's also something else that's quite interesting,
which is called Partial Dependence Plots that's actually
possible for all trees. Unfortunately, in scikit-learn, it’s
available only for the gradient boosting. So the idea here
is to not only see what parameters are important but how
will they influence the target. And so after you fit your
model, I'm using the Boston data set to the gradient
boosting regressor, there's a thing called plot partial
dependence, which gets the model and the training data set
and then the features for which I want to make the partial
dependence plots.

So here, I'm looking at the six most important features. The
feature importance is important according to the trees. So I
sort this and take the six most important ones.

---
# Partial Dependence Plots

.center[
![:scale 70%](images/feat_impo_part_dep.png)
]

???
And so this is what the partner dependence plots look like.
So this is the most important feature, the second most
important feature and so on. This is sort of the marginal
contribution of each feature. Basically, in each tree,
you're summing out the contribution of all the other
features, and you look only at what is the contribution of
this particular feature. In general, this would be hard to
compute. For tree-based models you can compute is a very
efficient way because you can basically just sum up over the
whole space by traversing the tree.

Here with increased room size, the price increases and you
can see how it increases and you can see that there's like a
big step function here. And you can see that for increased
LSTAT, the price decreases. For the others, there aren’t any
drastic effect. There seems to be some threshold for NOX.
And something funky going on DIS.

The question is, so I said for random forest, you can't find
out the direction of which way the feature influences. But
that was for the feature importance. So the feature
importance tells the impurity decrease. So here I also look
at the feature importance and they're just numbers so they
don't give you the direction. For both gradient boosting and
random forest, you can look at partial dependency plot, and
they will give you a non-parametric model of how a single
feature influences it. Unfortunately, it’s not available in
scikit-learn. You could do the same thing for random
forests. Because in the end, the way they predict is quite
similar.
---
# Partial Dependence
- Idea: Get marginal predictions given feature.
- How? "integrate out" other features using validation data
- Fast methods available for trees!
- Nonsensical for linear models.
- sklearn master: only gradient boosting
- Next sklearn: fast for gradient boosting, slow for everything else
- future: fast for all tree-based models

---
# Feature selection
---
# Why Select Features?

- Avoid overfitting (?)
- Faster prediction and training
- Less storage for model and dataset
- More interpretable model

???
There's a couple of reasons why you might want to feature
selection. One is to avoid overfitting and get a better
model. In practice, I have rarely seen that happen. It's not
usually what I would try to do to increase performance. If
I’m only interested in performance I probably would not try
to do automatic feature selection unless I think only a very
small subset of my feature is actually important. There's a
lot of increasing performance just by selecting only
important features.

What I think is more commonly, the reason to do automatic
feature selection is you want to shrink your model to make
faster predictions, to train your model faster, to store
fewer data and possibly to collect fewer data. If you're
collecting the data or to feature from some online process,
maybe it means you need fewer features, you need to store
less information. I think actually the top reason to do
feature selection is to have a more interpretable model. If
your model is smaller, if your model has 5 features instead
of 500, it's probably much easier for you to grasp what the
model does. If you can have a model that is as good with way
fewer features, it will be much easier to explain and most
people will be happy with it.
---
FIXME wrapper vs univariate
What happens with correlated features
compare to Kuhn book

---
# AutoML
---
# Formulating model-selection as Hyperparameter Optimization
- One big search, many conditional Hyper-Parameters
- Categorical, integer, continuous, conditional
- Different distributions
---
# CASH problem
- Find the best configuration
- Global optimization on complex (high-dim?) space
???
FIXME what does cash stand for?
---
# Issues with Grid-Search
- Need to define Grid
- Exponential in number of dims
---
class: middle
# Black-Box Search Procedures
`$$\Lambda^* = \arg\max_\Lambda f(\Lambda)$$`
Parameters $\Lambda$, model-evaluation $f$.
???
General optimization of unknown, non-differentiable f, possibly no-smooth.
NP Hard in general.
Function f is very slow to evaluate - think training a neural net for a week.
---
# Random Search
![:scale 100%](images/bergstra_random.jpeg)
---
# Bayesian Optimization, SMBO
.wide-left-column[

- fit 'cheap' probabilistic function to black-box

- pick next point using exploration / exploitation

- Implemented as acquisition function
]
.narrow-right-column[
.center[
![:scale 110%](images/smbo.png)
]]
---
# Successive Halving
- Given $n$ configuration and budget $B$
- pick $\eta=2$ or $\eta=3$ (wording follows 2)
- Each iteration, keep best halve of configurations
- after $k=\log_\eta(n)$ left with single configuration.
- initially allocate $\frac{B}{kn}$ to each configuration, double each iteration.
--
![:scale 70%](images/halving_algorithm.png)

---
# Dimensionality Reduction
---
# NMF & Outliers
---
# Text Data
---
# Topic models
---
# Word Embeddings
---
# Neural Networks
---
# Convolutional neural Networks
---
# Tricks for learning Deep Nets
---
# Practical Considerations for Neural Nets
---
# Time Series Data
---
class: middle
# Questions ?


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script>
    // Config Remark
    remark.macros['scale'] = function (percentage) {
        var url = this;
        return '<img src="' + url + '" style="width: ' + percentage + '" />';
    };
    config_remark = {
        highlightStyle: 'magula',
        highlightSpans: true,
        highlightLines: true,
        ratio: "16:9"
    };
      var slideshow = remark.create(config_remark);
    // Configure MathJax
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] /* removed 'code' entry*/
    }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
    </script>
  </body>
</html>
